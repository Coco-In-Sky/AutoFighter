{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "使用一对多的多分类策略，为每个状态建立单独的分类器，然后分别训练\n",
    "\n",
    "- 配置全局参数可以任意指定要训练的模型和对应的数据集\n",
    "    - 保存生成的数据集并且要支持从使用之前生成的数据集\n",
    "未完成部分\n",
    "- 配置全局参数可以针对指定的模型进行保存和读取\n",
    "    - 模型的保存和读取，要支持保存不同版本，同时留下模型的相关数据，方便后续选择出最优的模型\n",
    "- 拆分训练集和验证集\n",
    "- 配置整合函数main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from StatusChecker.TraditionalStatusChecker import TraditionalStatusChecker\n",
    "\n",
    "current_train_tag = TraditionalStatusChecker.ASC_STATUS_FIGHTING\n",
    "preset_positive_data_count_min = 100\n",
    "learn_rate = 0.1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "status_list = list(filter(lambda name: (name.startswith(\"ASC_STATUS_\") and name != 'ASC_STATUS_UNKNOWN'),\n",
    "                                  dir(TraditionalStatusChecker)))\n",
    "status_list = list(map(lambda name: getattr(TraditionalStatusChecker, name), status_list))\n",
    "\n",
    "print(status_list)\n",
    "print(f\"total {len(status_list)} status\")\n",
    "# tag_dic = status_list\n",
    "# tag_dic = [\"level_selection\",\n",
    "#            \"restore_sanity_medicine\",\n",
    "#            \"restore_sanity_stone\",\n",
    "#            \"team_up\",\n",
    "#            \"fighting\",\n",
    "#            \"battle_settlement\",\n",
    "#            \"annihilation_settlement\",\n",
    "#            \"level_up\"]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "根据训练目标选取数据集\n",
    "- 正向数据集选取目标标签，数量选取所有的或者指定阈值\n",
    "- 反向数据集选取其余的标签数据，数量取正向数据集长度平均至每个TAG，不够的暂时全上"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# 基础数据集路径\n",
    "train_set_base_path = os.path.join(os.getcwd(), '..', 'dataset', 'processed')\n",
    "\n",
    "#TODO 如果说数据量不平衡的话，最后那个数据会影响很多东西把\n",
    "def generate_train_data_list():\n",
    "    # 生成数据数量\n",
    "    data_count = {current_train_tag: min(\n",
    "        len(os.path.join(train_set_base_path, current_train_tag)),\n",
    "        preset_positive_data_count_min\n",
    "    )}\n",
    "    print(f\"needed positive data count: {data_count[current_train_tag]}\")\n",
    "    negative_data_count_need_each = data_count[current_train_tag] / (len(status_list) - 1)\n",
    "    print(f\"needed negative data count: {negative_data_count_need_each}\")\n",
    "    for status in status_list:\n",
    "        if status == current_train_tag:\n",
    "            continue\n",
    "        # 统计，取可用数据数量和当前所需目标数量的最小值\n",
    "        data_count[status] = min(\n",
    "            negative_data_count_need_each,\n",
    "            len(os.listdir(os.path.join(train_set_base_path, status)))\n",
    "        )\n",
    "    print(f\"decided data count use: {data_count}\")\n",
    "    # 根据数量开始处理数据路径\n",
    "    # 一个是要方便之后拿到TAG，一个是考虑要不要每次随机选取数据，不要每次都是开头的那些数据\n",
    "    # TAG可以随时从文件名拿到，随机选取数据用random生成一个不重复的数组，然后将它们作为实际的数据索引\n",
    "    # 跟上完整log以配合输出\n",
    "    # 正向数据生成\n",
    "    train_set_positive_data_set = random.choices(\n",
    "        os.listdir(\n",
    "            os.path.join(train_set_base_path, current_train_tag)\n",
    "        ), k=data_count[current_train_tag]\n",
    "    )\n",
    "    # 负面数据生成\n",
    "    train_set_negative_data_set = []\n",
    "    for status in status_list:\n",
    "        if status == current_train_tag:\n",
    "            continue\n",
    "        train_set_negative_data_set += random.choices(\n",
    "                os.listdir(\n",
    "                    os.path.join(train_set_base_path, status)\n",
    "                ), k=data_count[status]\n",
    "            )\n",
    "    # 输出本次数据集到log文件夹\n",
    "    from datetime import datetime\n",
    "    log_path = os.path.join(os.getcwd(), '..', 'log')\n",
    "    current_time = datetime.now()\n",
    "    print(f\"start log at {current_time}\")\n",
    "    with open(os.path.join(log_path, f\"train_data_set_list-{current_time}.log\"), \"w\") as log_file:\n",
    "        log_file.write(\"\\n===========positive start===============\\n\")\n",
    "        log_file.writelines(train_set_positive_data_set)\n",
    "        log_file.write(\"\\n===========positive end=================\\n\")\n",
    "        log_file.write(\"\\n===========negative start===============\\n\")\n",
    "        log_file.writelines(train_set_negative_data_set)\n",
    "        log_file.write(\"\\n===========negative end=================\\n\")\n",
    "    print(\"log finished\")\n",
    "    # 获得最终的数据列表\n",
    "    return train_set_positive_data_set + train_set_negative_data_set\n",
    "\n",
    "# 用于生成数据对应的TAG\n",
    "def get_image_tag(filename:str) -> int:\n",
    "    for index, tag in enumerate(status_list):\n",
    "        if tag in filename:\n",
    "            return index\n",
    "\n",
    "def generate_target_list(train_file_list:list):\n",
    "\n",
    "    # 取得对应的标签数据\n",
    "    return list(map(get_image_tag, train_file_list))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "配置 DataSet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def default_loader(image_name:str):\n",
    "    image_loaded = cv.imread(\n",
    "        os.path.join(train_set_base_path, status_list[get_image_tag(image_name)], image_name),\n",
    "        cv.IMREAD_GRAYSCALE)\n",
    "    print(f\"load image {image_name}\")\n",
    "    # print(image_tensor.size())\n",
    "    return image_loaded\n",
    "\n",
    "class TrainSet(Dataset):\n",
    "    def __init__(self, image_name_list:list, targets:list, loader=default_loader)-> None:\n",
    "        self.images = image_name_list\n",
    "        self.targets = targets\n",
    "        self.loader = loader\n",
    "    def __getitem__(self, index: int):\n",
    "        image = self.loader(self.images[index])\n",
    "        image = torch.tensor(image, device=torch.device('cuda'), dtype=torch.float32).unsqueeze(0)\n",
    "        target = self.targets[index]\n",
    "        target = torch.tensor(target, dtype=torch.long, device=torch.device('cuda'))\n",
    "        return image, target\n",
    "    def __len__(self)-> int:\n",
    "        return len(self.images)\n",
    "\n",
    "def get_train_set(train_file_list:list, target_list:list) -> Dataset:\n",
    "\n",
    "    train_set = TrainSet(train_file_list, target_list)\n",
    "    print(train_set)\n",
    "    print(train_set.__getitem__(3))\n",
    "    print(train_set.__len__())\n",
    "    return train_set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "构建 DataLoader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "def get_train_loader(train_set:Dataset) -> DataLoader:\n",
    "    train_loader = DataLoader(train_set, batch_size=1, shuffle=True)\n",
    "    print(train_loader)\n",
    "    print(iter(train_loader).next()[0].size())\n",
    "    print(iter(train_loader).next()[1].size())\n",
    "    return train_loader\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "定义模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Lambda(torch.nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "    def forward(self, xb):\n",
    "        return self.func(xb)\n",
    "\n",
    "def get_module():\n",
    "    module = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=3),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=3),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=3),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.AdaptiveAvgPool2d([16, 36]),\n",
    "        Lambda(lambda xb: xb.view(-1, 10*16*36)),\n",
    "        torch.nn.Linear(10*16*36, 16*36),\n",
    "        torch.nn.Linear(16*36, 36),\n",
    "        torch.nn.Linear(36, 1),\n",
    "        torch.nn.Softmax(),\n",
    "    )\n",
    "    optimize = torch.optim.SGD(module.parameters(), lr=learn_rate)\n",
    "    return module, optimize\n",
    "\n",
    "def get_loss_function():\n",
    "    loss_func = torch.nn.functional.binary_cross_entropy\n",
    "    return loss_func"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "fit() 函数以及模型的保存和恢复"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "def fit(epochs:int, module:torch.nn.Module,\n",
    "        loss_function, optimize, train_loader, validate_loader):\n",
    "    index = 0\n",
    "    for epoch in range(epochs):\n",
    "        for xb, yb in train_loader:\n",
    "            index = index + 1\n",
    "            if index%300 is 0:\n",
    "                print(f\"start index {index} data\")\n",
    "            loss_batch(module, loss_function, xb, yb, optimize)\n",
    "        module.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(module, loss_function, xb, yb)\n",
    "                  for xb, yb in validate_loader]\n",
    "            )\n",
    "        val_loss = numpy.sum(numpy.multiply(losses, nums)) / numpy.sum(nums)\n",
    "        print(epoch, val_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "准备启动"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def main():\n",
    "\n",
    "train_file_list = generate_train_data_list()\n",
    "target_list = generate_target_list(train_file_list)\n",
    "print(status_list)\n",
    "print(len(train_file_list))\n",
    "print(train_file_list[0])\n",
    "print(len(target_list))\n",
    "print(target_list[0])\n",
    "\n",
    "train_set = get_train_set(train_file_list, target_list)\n",
    "\n",
    "train_loader = get_train_loader(train_set)\n",
    "\n",
    "optimize, module = get_module()\n",
    "loss_function = get_loss_function()\n",
    "\n",
    "fit(epochs=1, module=module, loss_function=loss_function,\n",
    "    optimize=optimize, train_loader=train_loader, validate_loader=None)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}