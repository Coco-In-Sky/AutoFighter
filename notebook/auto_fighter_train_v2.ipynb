{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "使用一对多的多分类策略，为每个状态建立单独的分类器，然后分别训练\n",
    "\n",
    "- 配置全局参数可以任意指定要训练的模型和对应的数据集\n",
    "    - 保存生成的数据集并且要支持从使用之前生成的数据集\n",
    "未完成部分\n",
    "- 配置全局参数可以针对指定的模型进行保存和读取\n",
    "    - 模型的保存和读取，要支持保存不同版本，同时留下模型的相关数据，方便后续选择出最优的模型\n",
    "- 拆分训练集和验证集\n",
    "- 配置整合函数main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup finished, timestap is 2020-8-3-1-27-19\n"
     ]
    }
   ],
   "source": [
    "from StatusChecker.TraditionalStatusChecker import TraditionalStatusChecker\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "current_train_tag = TraditionalStatusChecker.ASC_STATUS_FIGHTING\n",
    "preset_positive_data_count_min = 100\n",
    "learn_rate = 0.01\n",
    "\n",
    "\n",
    "# 重新生成datetime.now()\n",
    "log_time = datetime.now()\n",
    "current_time = f\"{log_time.year}-{log_time.month}-{log_time.day}\" \\\n",
    "               f\"-{log_time.hour}-{log_time.minute}-{log_time.second}\"\n",
    "# 日志保存位置\n",
    "log_path = os.path.join(os.getcwd(), '..', 'log')\n",
    "# 训练集日志保存位置\n",
    "train_set_list_log_base_path = os.path.join(log_path, 'dataset_list', current_time)\n",
    "os.makedirs(train_set_list_log_base_path, exist_ok=True)\n",
    "\n",
    "# 基础数据集路径\n",
    "train_set_base_path = os.path.join(os.getcwd(), '..', 'dataset', 'processed')\n",
    "# 验证集路径\n",
    "validation_data_base_path = os.path.join(os.getcwd(), '..', 'dataset', 'validation')\n",
    "# 数据集列表保存路径\n",
    "data_set_list_base_path = train_set_list_log_base_path\n",
    "os.makedirs(data_set_list_base_path, exist_ok=True)\n",
    "\n",
    "# 模型检查点保存路径\n",
    "module_checkpoint_base_path = os.path.join(log_path, 'model', current_time)\n",
    "os.makedirs(module_checkpoint_base_path, exist_ok=True)\n",
    "# 模型训练数据保存路径\n",
    "module_train_data_base_path = os.path.join(log_path, 'train_log', current_time)\n",
    "os.makedirs(module_train_data_base_path, exist_ok=True)\n",
    "\n",
    "\n",
    "print(f\"setup finished, timestap is {current_time}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['annihilation_settlement', 'battle_settlement', 'fighting', 'level_selection', 'level_up', 'restore_sanity_medicine', 'restore_sanity_stone', 'team_up']\n",
      "total 8 status\n"
     ]
    }
   ],
   "source": [
    "status_list = list(filter(lambda name: (name.startswith(\"ASC_STATUS_\") and name != 'ASC_STATUS_UNKNOWN'),\n",
    "                                  dir(TraditionalStatusChecker)))\n",
    "status_list = list(map(lambda name: getattr(TraditionalStatusChecker, name), status_list))\n",
    "\n",
    "print(status_list)\n",
    "print(f\"total {len(status_list)} status\")\n",
    "# tag_dic = status_list\n",
    "# tag_dic = [\"level_selection\",\n",
    "#            \"restore_sanity_medicine\",\n",
    "#            \"restore_sanity_stone\",\n",
    "#            \"team_up\",\n",
    "#            \"fighting\",\n",
    "#            \"battle_settlement\",\n",
    "#            \"annihilation_settlement\",\n",
    "#            \"level_up\"]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "根据训练目标选取数据集\n",
    "- 正向数据集选取目标标签，数量选取所有的或者指定阈值\n",
    "- 反向数据集选取其余的标签数据，数量取正向数据集长度平均至每个TAG，不够的暂时全上"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "#TODO 如果说数据量不平衡的话，最后那个数据会影响很多东西把\n",
    "def generate_train_data_list():\n",
    "    # 生成数据数量\n",
    "    data_count = {current_train_tag: min(\n",
    "        len(os.listdir(os.path.join(train_set_base_path, current_train_tag))),\n",
    "        preset_positive_data_count_min\n",
    "    )}\n",
    "    print(f\"needed positive data count: {data_count[current_train_tag]}\")\n",
    "    negative_data_count_need_each = data_count[current_train_tag] / (len(status_list) - 1)\n",
    "    print(f\"needed negative data count: {negative_data_count_need_each}\")\n",
    "    for status in status_list:\n",
    "        if status == current_train_tag:\n",
    "            continue\n",
    "        # 统计，取可用数据数量和当前所需目标数量的最小值\n",
    "        data_count[status] = min(\n",
    "            negative_data_count_need_each,\n",
    "            len(os.listdir(os.path.join(train_set_base_path, status)))\n",
    "        )\n",
    "    print(f\"decided data count use: {data_count}\")\n",
    "    # 根据数量开始处理数据路径\n",
    "    # 一个是要方便之后拿到TAG，一个是考虑要不要每次随机选取数据，不要每次都是开头的那些数据\n",
    "    # TAG可以随时从文件名拿到，随机选取数据用random生成一个不重复的数组，然后将它们作为实际的数据索引\n",
    "    # 跟上完整log以配合输出\n",
    "    # 正向数据生成\n",
    "    train_set_positive_data_set = random.choices(\n",
    "        os.listdir(\n",
    "            os.path.join(train_set_base_path, current_train_tag)\n",
    "        ), k=data_count[current_train_tag]\n",
    "    )\n",
    "    # 负面数据生成\n",
    "    train_set_negative_data_set = []\n",
    "    for status in status_list:\n",
    "        if status == current_train_tag:\n",
    "            continue\n",
    "        train_set_negative_data_set += random.choices(\n",
    "                os.listdir(\n",
    "                    os.path.join(train_set_base_path, status)\n",
    "                ), k=int(data_count[status])\n",
    "            )\n",
    "    # 输出本次数据集到log文件夹\n",
    "\n",
    "    print(f\"start log at {current_time}, status: {current_train_tag}\")\n",
    "    with open(os.path.join(train_set_list_log_base_path, f\"train-data-set-list-{current_train_tag}.log\"), \"w\") as log_file:\n",
    "        log_file.write(\"\\n===========positive start===============\\n\")\n",
    "        log_file.writelines(train_set_positive_data_set)\n",
    "        log_file.write(\"\\n===========positive end=================\\n\")\n",
    "        log_file.write(\"\\n===========negative start===============\\n\")\n",
    "        log_file.writelines(train_set_negative_data_set)\n",
    "        log_file.write(\"\\n===========negative end=================\\n\")\n",
    "    print(\"log finished\")\n",
    "    # 获得最终的数据列表\n",
    "    data_set = train_set_positive_data_set + train_set_negative_data_set\n",
    "    # 保存生成的数据列表\n",
    "\n",
    "    train_set_list_filename = f\"train-set-file-list-{current_train_tag}.pt\"\n",
    "    torch.save(data_set, os.path.join(data_set_list_base_path, train_set_list_filename))\n",
    "    print(f\"saved train set list in train-set-file-list.pt\")\n",
    "    return data_set\n",
    "\n",
    "# 从本地读取已生成的数据列表\n",
    "def load_data_list(filename: str):\n",
    "    return torch.load(os.path.join(data_set_list_base_path, filename))\n",
    "\n",
    "# 用于生成数据对应的TAG\n",
    "def get_image_tag(filename:str) -> int:\n",
    "    # for index, tag in enumerate(status_list):\n",
    "    #     if tag in filename:\n",
    "    #         return index\n",
    "    for tag in status_list:\n",
    "        if tag in filename:\n",
    "            if tag == current_train_tag:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "def get_image_tag_origin(filename:str) -> str:\n",
    "    for tag in status_list:\n",
    "        if tag in filename:\n",
    "            return tag\n",
    "\n",
    "def generate_target_list(train_file_list:list):\n",
    "\n",
    "    # 取得对应的标签数据\n",
    "    return list(map(get_image_tag, train_file_list))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "配置 DataSet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def default_loader(image_name:str):\n",
    "    image_loaded = cv.imread(\n",
    "        os.path.join(train_set_base_path, get_image_tag_origin(image_name), image_name),\n",
    "        cv.IMREAD_GRAYSCALE)\n",
    "    # print(f\"load image {image_name}\")\n",
    "    # print(image_tensor.size())\n",
    "    return image_loaded\n",
    "\n",
    "def validation_path_loader(image_name:str):\n",
    "    image_loaded = cv.imread(\n",
    "        os.path.join(validation_data_base_path, image_name),\n",
    "        cv.IMREAD_GRAYSCALE)\n",
    "    # print(f\"load image {image_name}\")\n",
    "    # print(image_tensor.size())\n",
    "    return image_loaded\n",
    "\n",
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self, image_name_list:list, targets:list, loader=default_loader)-> None:\n",
    "        self.images = image_name_list\n",
    "        self.targets = targets\n",
    "        self.loader = loader\n",
    "    def __getitem__(self, index: int):\n",
    "        image = self.loader(self.images[index])\n",
    "        image = torch.tensor(image, device=torch.device('cuda'), dtype=torch.float32).unsqueeze(0)\n",
    "        target = self.targets[index]\n",
    "        target = torch.tensor(target, dtype=torch.float32, device=torch.device('cuda'))\n",
    "        return image, target\n",
    "    def __len__(self)-> int:\n",
    "        return len(self.images)\n",
    "\n",
    "def get_train_set(train_file_list: list) -> Dataset:\n",
    "    target_list = generate_target_list(train_file_list)\n",
    "\n",
    "    print(len(train_file_list))\n",
    "    print(train_file_list[0])\n",
    "    print(len(target_list))\n",
    "    print(target_list[0])\n",
    "\n",
    "    train_set = CustomDataSet(train_file_list, target_list)\n",
    "    print(train_set)\n",
    "    print(train_set.__getitem__(3))\n",
    "    print(train_set.__len__())\n",
    "    return train_set\n",
    "\n",
    "\n",
    "def get_validation_set()->Dataset:\n",
    "    validation_file_list = os.listdir(validation_data_base_path)\n",
    "    target_list = list(map(get_image_tag, validation_file_list))\n",
    "    validation_set = CustomDataSet(validation_file_list, target_list, loader=validation_path_loader)\n",
    "    return validation_set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "构建 DataLoader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "def get_train_loader(train_set:Dataset) -> DataLoader:\n",
    "    train_loader = DataLoader(train_set, batch_size=1, shuffle=True)\n",
    "    print(train_loader)\n",
    "    print(iter(train_loader).next()[0].size())\n",
    "    print(iter(train_loader).next()[1].size())\n",
    "    return train_loader\n",
    "\n",
    "def get_validation_loader(validation_set:Dataset) -> DataLoader:\n",
    "    validation_loader = DataLoader(validation_set, batch_size=1, shuffle=True)\n",
    "    print(validation_loader)\n",
    "    print(iter(validation_loader).next()[0].size())\n",
    "    print(iter(validation_loader).next()[1].size())\n",
    "    return validation_loader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "定义模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Lambda(torch.nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "    def forward(self, xb):\n",
    "        return self.func(xb)\n",
    "\n",
    "def get_module():\n",
    "    module = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=3),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=3),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=3),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.AdaptiveAvgPool2d([16, 36]),\n",
    "        Lambda(lambda xb: xb.view(-1, 10*16*36)),\n",
    "        torch.nn.Linear(10*16*36, 16*36),\n",
    "        torch.nn.Linear(16*36, 36),\n",
    "        torch.nn.Linear(36, 1),\n",
    "        torch.nn.Softmax(),\n",
    "    ).to(torch.device('cuda'))\n",
    "    optimize = torch.optim.SGD(module.parameters(), lr=learn_rate)\n",
    "    return module, optimize\n",
    "\n",
    "def get_loss_function():\n",
    "    loss_func = torch.nn.functional.binary_cross_entropy\n",
    "    return loss_func"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "fit() 函数以及模型的保存和恢复"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    # return loss.item(), len(xb)\n",
    "    return loss, 1\n",
    "def fit(epochs:int, module:torch.nn.Module,\n",
    "        loss_function, optimize, train_loader, validate_loader):\n",
    "    index = 0\n",
    "    with open(os.path.join(module_train_data_base_path, 'train.log'), 'w') as log:\n",
    "        for epoch in range(epochs):\n",
    "            module.train()\n",
    "            for xb, yb in train_loader:\n",
    "                index = index + 1\n",
    "                if index%20 == 0:\n",
    "                    print(f\"start index {index} data\")\n",
    "                train_loss, _ = loss_batch(module, loss_function, xb, yb, optimize)\n",
    "                if index%20 == 0:\n",
    "                    log.write(f\"epoch: {epoch}, index: {index}, train_loss: {train_loss}\\n\")\n",
    "            module.eval()\n",
    "            with torch.no_grad():\n",
    "                losses, nums = zip(\n",
    "                    *[loss_batch(module, loss_function, xb, yb)\n",
    "                      for xb, yb in validate_loader]\n",
    "                )\n",
    "            validation_loss = numpy.sum(numpy.multiply(losses, nums)) / numpy.sum(nums)\n",
    "            print(epoch, validation_loss)\n",
    "            log.write(f\"epoch: {epoch}, index: {index}, val_loss: {validation_loss}\\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "准备启动"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['annihilation_settlement', 'battle_settlement', 'fighting', 'level_selection', 'level_up', 'restore_sanity_medicine', 'restore_sanity_stone', 'team_up']\n",
      "needed positive data count: 100\n",
      "needed negative data count: 14.285714285714286\n",
      "decided data count use: {'fighting': 100, 'annihilation_settlement': 14.285714285714286, 'battle_settlement': 14.285714285714286, 'level_selection': 14.285714285714286, 'level_up': 7, 'restore_sanity_medicine': 14.285714285714286, 'restore_sanity_stone': 14.285714285714286, 'team_up': 14.285714285714286}\n",
      "start log at 2020-8-3-1-27-19, status: fighting\n",
      "log finished\n",
      "saved train set list in train-set-file-list.pt\n",
      "191\n",
      "fighting-6554.png\n",
      "191\n",
      "1\n",
      "<__main__.CustomDataSet object at 0x0000024F8200CDF0>\n",
      "(tensor([[[ 42.,  40.,  41.,  ...,  40.,  40.,  41.],\n",
      "         [ 39.,  38.,  39.,  ...,  39.,  38.,  39.],\n",
      "         [ 41.,  39.,  40.,  ...,  40.,  39.,  41.],\n",
      "         ...,\n",
      "         [ 39.,  38.,  39.,  ..., 103., 100.,  97.],\n",
      "         [ 38.,  38.,  38.,  ..., 103., 100.,  97.],\n",
      "         [ 37.,  36.,  37.,  ...,  97.,  93.,  95.]]], device='cuda:0'), tensor(1., device='cuda:0'))\n",
      "191\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x0000024F83797DC0>\n",
      "torch.Size([1, 1, 900, 1600])\n",
      "torch.Size([1])\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x0000024F83797D30>\n",
      "torch.Size([1, 1, 900, 1600])\n",
      "torch.Size([1])\n",
      "start index 20 data\n",
      "start index 40 data\n",
      "start index 60 data\n",
      "start index 80 data\n",
      "start index 100 data\n",
      "start index 120 data\n",
      "start index 140 data\n",
      "start index 160 data\n",
      "start index 180 data\n",
      "0 tensor(87.5000, device='cuda:0')\n",
      "start index 200 data\n",
      "start index 220 data\n",
      "start index 240 data\n",
      "start index 260 data\n",
      "start index 280 data\n",
      "start index 300 data\n",
      "start index 320 data\n",
      "start index 340 data\n",
      "start index 360 data\n",
      "start index 380 data\n",
      "1 tensor(87.5000, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\Anaconda3\\envs\\auto_fight\\lib\\site-packages\\torch\\nn\\modules\\container.py:100: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n",
      "<ipython-input-7-468beef88a3a>:5: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])) is deprecated. Please ensure they have the same size.\n",
      "  loss = loss_func(model(xb), yb)\n"
     ]
    }
   ],
   "source": [
    "# def main():\n",
    "print(status_list)\n",
    "\n",
    "\n",
    "train_file_list = generate_train_data_list()\n",
    "# train_file_list = load_data_list('')\n",
    "train_set = get_train_set(train_file_list)\n",
    "validation_set = get_validation_set()\n",
    "train_loader = get_train_loader(train_set)\n",
    "validation_loader = get_validation_loader(validation_set)\n",
    "module, optimize  = get_module()\n",
    "loss_function = get_loss_function()\n",
    "\n",
    "fit(epochs=2, module=module, loss_function=loss_function,\n",
    "    optimize=optimize, train_loader=train_loader, validate_loader=validation_loader)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "conda-env-auto_fight-py",
   "language": "python",
   "display_name": "Python [conda env:auto_fight]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}